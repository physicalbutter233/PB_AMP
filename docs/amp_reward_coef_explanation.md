# amp_reward_coef 参数详解

本文档详细解释 `amp_reward_coef` 参数的含义和作用。

## 一、参数定义

**参数名**: `amp_reward_coef`  
**类型**: `float`  
**默认值**: 通常为 `0.3`  
**当前配置**: `0.01`

**含义**: AMP 奖励的缩放系数，决定 AMP 奖励的最大值。

---

## 二、在代码中的使用

### 2.1 代码位置

**文件**: `rsl_rl/rsl_rl/modules/discriminator.py`  
**函数**: `predict_amp_reward()`  
**行号**: 第 124 行

```python
def predict_amp_reward(self, state, next_state, task_reward, normalizer=None):
    # ... 前面的代码 ...
    
    # 1. 计算判别器输出
    d = self.amp_linear(self.trunk(torch.cat([state, next_state], dim=-1)))
    
    # 2. 计算 AMP 奖励（关键公式）
    reward = self.amp_reward_coef * torch.clamp(
        1 - (1 / 4) * torch.square(d - 1), 
        min=0
    )
    
    # 3. 与 Task 奖励插值（如果启用）
    if self.task_reward_lerp > 0:
        reward = self._lerp_reward(reward, task_reward.unsqueeze(-1))
    
    return reward.squeeze(), d
```

### 2.2 公式解析

**AMP 奖励计算公式**:
```
AMP奖励 = amp_reward_coef × clamp(1 - 0.25 × (d - 1)², min=0)
```

**各部分含义**:

1. **`d`**: 判别器输出（logits）
   - 范围：理论上可以是任意值
   - 含义：策略动作与专家动作的相似度
   - `d = 1` 表示完美匹配专家
   - `d` 远离 1 表示与专家差异大

2. **`(d - 1)²`**: 与目标值的平方差
   - 当 `d = 1` 时，值为 0（完美匹配）
   - `d` 越远离 1，值越大

3. **`1 - 0.25 × (d - 1)²`**: 奖励基础值
   - 当 `d = 1` 时，值为 `1 - 0 = 1`（最大值）
   - 当 `d = 3` 或 `d = -1` 时，值为 `1 - 1 = 0`（最小值）
   - 范围：`[0, 1]`

4. **`clamp(..., min=0)`**: 确保非负
   - 如果计算结果 < 0，则设为 0

5. **`amp_reward_coef × ...`**: 缩放系数
   - 将 `[0, 1]` 的范围缩放到 `[0, amp_reward_coef]`
   - **这就是 AMP 奖励的最大值**

---

## 三、参数作用

### 3.1 决定 AMP 奖励的最大值

**当 `d = 1`（完美匹配专家）时**:
```
AMP奖励 = amp_reward_coef × 1 = amp_reward_coef
```

**示例**:
- `amp_reward_coef = 0.3` → AMP 奖励最大 = 0.3
- `amp_reward_coef = 0.1` → AMP 奖励最大 = 0.1
- `amp_reward_coef = 0.01` → AMP 奖励最大 = 0.01

### 3.2 影响 AMP 奖励的范围

| amp_reward_coef | AMP 奖励范围 | 说明 |
|-----------------|--------------|------|
| 0.3 | [0, 0.3] | 原始配置，AMP 奖励较大 |
| 0.2 | [0, 0.2] | 降低 AMP 奖励 |
| 0.1 | [0, 0.1] | 进一步降低 |
| 0.01 | [0, 0.01] | 大幅降低，使 AMP 占比很小 |

### 3.3 影响最终奖励的占比

**最终奖励公式**:
```
最终奖励 = (1 - lerp) × AMP奖励 + lerp × Task奖励
```

**示例**（假设 Task 奖励 = 0.01/步）:

| amp_reward_coef | AMP奖励最大 | lerp | AMP贡献 | Task贡献 | AMP占比 |
|-----------------|-------------|------|---------|----------|---------|
| 0.3 | 0.3 | 0.7 | 0.09 | 0.007 | 92.8% |
| 0.1 | 0.1 | 0.7 | 0.03 | 0.007 | 81.1% |
| 0.01 | 0.01 | 0.96 | 0.0004 | 0.0096 | 4.0% |

---

## 四、参数调整的影响

### 4.1 增加 `amp_reward_coef`

**效果**:
- ✅ AMP 奖励最大值增加
- ✅ 策略更倾向于模仿专家动作
- ❌ 可能忽略任务目标（移动、速度跟踪等）

**适用场景**:
- 希望学习自然动作风格
- 任务奖励已经足够强
- 专家数据质量很高

### 4.2 降低 `amp_reward_coef`

**效果**:
- ✅ AMP 奖励最大值降低
- ✅ 策略更关注任务目标
- ✅ 减少对专家动作的过度模仿
- ❌ 动作可能不够自然

**适用场景**:
- 机器人不愿意移动（当前问题）
- 需要优先完成任务目标
- 专家数据可能包含不期望的行为（如静止）

### 4.3 当前配置（0.01）

**目的**: 使 AMP 奖励占比降至 4%

**效果**:
- AMP 奖励最大值 = 0.01（很小）
- 即使完美匹配专家，AMP 奖励也只有 0.01
- 配合 `amp_task_reward_lerp = 0.96`，AMP 占比约 4%

---

## 五、与其他参数的关系

### 5.1 与 `amp_task_reward_lerp` 的关系

**`amp_task_reward_lerp`**: Task 奖励在最终奖励中的权重

**组合效果**:

| amp_reward_coef | amp_task_reward_lerp | AMP占比 | Task占比 |
|-----------------|---------------------|---------|----------|
| 0.3 | 0.7 | ~95% | ~5% |
| 0.2 | 0.85 | ~95% | ~5% |
| 0.1 | 0.9 | ~90% | ~10% |
| 0.01 | 0.96 | ~4% | ~96% |

**注意**: 即使 `lerp` 很大（如 0.96），如果 `amp_reward_coef` 太大，AMP 奖励仍然可能占主导。

### 5.2 与 Task 奖励的关系

**Task 奖励大小**也会影响占比：

**如果 Task 奖励很小（0.001/步）**:
- 即使 `amp_reward_coef = 0.01`，AMP 奖励（0.01）仍然比 Task 奖励（0.001）大 10 倍
- 需要进一步降低 `amp_reward_coef` 或增加 Task 奖励权重

**如果 Task 奖励较大（0.01/步）**:
- `amp_reward_coef = 0.01` 时，AMP 和 Task 奖励相当
- 配合 `lerp = 0.96`，可以实现 AMP 占比 4%

---

## 六、实际计算示例

### 6.1 示例1: 完美匹配专家

**假设**:
- `d = 1`（完美匹配）
- `amp_reward_coef = 0.01`
- `task_reward = 0.01`
- `amp_task_reward_lerp = 0.96`

**计算**:
```
AMP奖励 = 0.01 × clamp(1 - 0.25 × (1-1)², min=0)
       = 0.01 × 1
       = 0.01

最终奖励 = 0.04 × 0.01 + 0.96 × 0.01
        = 0.0004 + 0.0096
        = 0.01

AMP占比 = 0.0004 / 0.01 = 4%  ✅
Task占比 = 0.0096 / 0.01 = 96%  ✅
```

### 6.2 示例2: 不匹配专家

**假设**:
- `d = 3`（不匹配）
- `amp_reward_coef = 0.01`
- `task_reward = 0.01`
- `amp_task_reward_lerp = 0.96`

**计算**:
```
AMP奖励 = 0.01 × clamp(1 - 0.25 × (3-1)², min=0)
       = 0.01 × clamp(1 - 1, min=0)
       = 0.01 × 0
       = 0.0

最终奖励 = 0.04 × 0.0 + 0.96 × 0.01
        = 0.0 + 0.0096
        = 0.0096

AMP占比 = 0.0 / 0.0096 = 0%
Task占比 = 0.0096 / 0.0096 = 100%
```

---

## 七、总结

### 7.1 关键点

1. **`amp_reward_coef` 决定 AMP 奖励的最大值**
   - 当 `d = 1`（完美匹配）时，AMP 奖励 = `amp_reward_coef`

2. **影响 AMP 奖励的范围**
   - AMP 奖励范围：`[0, amp_reward_coef]`

3. **影响最终奖励的占比**
   - 配合 `amp_task_reward_lerp` 使用
   - 需要根据 Task 奖励大小调整

4. **当前配置（0.01）的目的**
   - 使 AMP 奖励最大值很小（0.01）
   - 配合 `lerp = 0.96`，实现 AMP 占比 4%

### 7.2 调整建议

**如果 AMP 占比仍然太高**:
- 进一步降低 `amp_reward_coef`（如 0.005）
- 或增加 Task 奖励权重（增加移动奖励权重）

**如果动作不够自然**:
- 略微增加 `amp_reward_coef`（如 0.02）
- 或降低 `amp_task_reward_lerp`（如 0.9）

**如果训练不稳定**:
- 保持当前配置
- 调整学习率或其他超参数
