# 训练日志分析（Iteration 70）- 新配置验证

基于新训练日志（iteration 70）的奖励分析，验证新配置是否生效。

## 一、训练数据

**日志数据**:
- Mean reward: 3.43
- Mean episode length: 47.43 步
- Episode_Reward/track_lin_vel_xy_exp: 0.1138
- Episode_Reward/track_ang_vel_z_exp: 0.1199
- 其他奖励项（惩罚项）总和: -0.0643

**预期配置**:
- `amp_reward_coef = 0.01`
- `amp_task_reward_lerp = 0.96` (96% Task, 4% AMP)

---

## 二、奖励计算

### 2.1 每步平均奖励

```
每步平均奖励 = 3.43 / 47.43 ≈ 0.0723
```

### 2.2 每步 Task 奖励

**Task 奖励项（累积值）**:
```
track_lin_vel_xy_exp:  0.1138
track_ang_vel_z_exp:   0.1199
─────────────────────────────
正奖励总和:             0.2337

lin_vel_z_l2:         -0.0020
ang_vel_xy_l2:        -0.0362
flat_orientation_l2:  -0.0034
energy:               -0.0000
dof_acc_l2:           -0.0118
action_rate_l2:       -0.0041
undesired_contacts:   -0.0012
feet_slide:           -0.0013
feet_force:           -0.0043
─────────────────────────────
惩罚项总和:            -0.0643

Task奖励总和（累积）:  0.2337 - 0.0643 = 0.1694
```

**每步 Task 奖励**:
```
每步Task奖励 = 0.1694 / 47.43 ≈ 0.00357
```

### 2.3 反推 AMP 奖励和配置

**假设使用新配置**: `amp_task_reward_lerp = 0.96`, `amp_reward_coef = 0.01`

**计算**:
```
最终奖励 = 0.04 × AMP奖励 + 0.96 × Task奖励
0.0723 = 0.04 × AMP奖励 + 0.96 × 0.00357
0.0723 = 0.04 × AMP奖励 + 0.00343
AMP奖励 = (0.0723 - 0.00343) / 0.04 ≈ 1.722  ❌ 不合理！
```

**问题**: AMP 奖励 1.722 远大于 `amp_reward_coef = 0.01` 的最大值（0.01），说明**训练使用的不是新配置**。

**假设使用旧配置**: `amp_task_reward_lerp = 0.7`, `amp_reward_coef = 0.3`

**计算**:
```
0.0723 = 0.3 × AMP奖励 + 0.7 × 0.00357
0.0723 = 0.3 × AMP奖励 + 0.00250
AMP奖励 = (0.0723 - 0.00250) / 0.3 ≈ 0.233
```

**验证**:
```
AMP贡献 = 0.3 × 0.233 = 0.0699
Task贡献 = 0.7 × 0.00357 = 0.00250
最终奖励 = 0.0699 + 0.00250 = 0.0724  ✅ 匹配！

AMP占比 = 0.0699 / 0.0724 ≈ 96.5%
Task占比 = 0.00250 / 0.0724 ≈ 3.5%
```

---

## 三、结论

### 3.1 配置状态

**训练使用的配置**:
- `amp_reward_coef = 0.3`（旧配置）
- `amp_task_reward_lerp = 0.7`（旧配置）

**奖励占比**:
- **AMP 占比: 96.5%**
- **Task 占比: 3.5%**

### 3.2 为什么新配置没有生效？

**可能原因**:
1. **训练从检查点恢复**: 虽然配置参数会使用新值，但可能训练脚本使用了 `--resume` 或 `resume=True`
2. **Python 模块缓存**: Python 缓存了旧的配置类
3. **配置文件没有重新加载**: 训练脚本在配置修改前启动

### 3.3 对比分析

| 项目 | Iteration 249 | Iteration 70 | 变化 |
|------|---------------|--------------|------|
| Mean reward | 12.23 | 3.43 | ↓ 72% |
| Episode length | 83.09 | 47.43 | ↓ 43% |
| Task奖励/步 | 0.00641 | 0.00357 | ↓ 44% |
| AMP占比 | ~97% | ~96.5% | 基本不变 |

**关键发现**:
- ❌ 新配置**没有生效**（仍然使用旧配置）
- ⚠️ Task 奖励降低了（可能是训练早期）
- ⚠️ Episode length 大幅降低（47步），说明训练不稳定或频繁重置

---

## 四、如何确保新配置生效？

### 4.1 检查训练启动日志

**查看训练启动时的输出**，应该看到：
```
================================================================================
[AmpOnPolicyRunner] AMP 配置:
  amp_reward_coef = 0.01
  amp_task_reward_lerp = 0.96
================================================================================
```

**如果没有看到这些输出，说明**:
- 训练脚本可能使用了旧代码（没有调试输出）
- 或者配置没有正确加载

### 4.2 确保新配置生效的步骤

**步骤1: 停止当前训练**

**步骤2: 清除 Python 缓存**
```bash
find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null
find . -name "*.pyc" -delete
```

**步骤3: 检查配置文件**
```bash
python3 legged_lab/scripts/check_amp_config.py
```

**步骤4: 重新开始训练（不使用检查点）**
```bash
# 确保 walk_cfg.py 中 resume = False
python legged_lab/scripts/train.py --task=walk
```

**步骤5: 查看训练启动输出**
- 应该看到配置打印输出
- 确认 `amp_reward_coef = 0.01` 和 `amp_task_reward_lerp = 0.96`

---

## 五、如果新配置生效，预期结果

**如果使用新配置**（`coef=0.01, lerp=0.96`）:

**假设 Task 奖励 = 0.00357/步**:
```
AMP奖励最大 = 0.01
最终奖励 = 0.04 × 0.01 + 0.96 × 0.00357
        = 0.0004 + 0.00343
        = 0.00383

AMP占比 = 0.0004 / 0.00383 ≈ 10.4%
Task占比 = 0.00343 / 0.00383 ≈ 89.6%
```

**但实际 Mean reward = 3.43，每步 = 0.0723**，说明：
- 要么配置没有生效（使用旧配置）
- 要么 Task 奖励实际更大

---

## 六、总结

### 6.1 当前状态

- **AMP 占比: 约 96.5%**（仍然很高）
- **训练使用的可能是旧配置**（`coef=0.3, lerp=0.7`）
- **新配置可能没有生效**

### 6.2 下一步行动

1. **检查训练启动日志**，看是否有配置打印输出
2. **如果没有看到配置输出**，说明使用了旧代码，需要：
   - 停止训练
   - 清除缓存
   - 重新启动训练
3. **如果看到配置输出但值不对**，检查配置文件是否正确

### 6.3 验证方法

**从训练日志计算**:
```
如果使用新配置（lerp=0.96, coef=0.01）:
- AMP奖励最大 = 0.01
- 如果最终奖励 = 0.0723，AMP占比应该很小（< 20%）

如果使用旧配置（lerp=0.7, coef=0.3）:
- AMP奖励可能很大（0.2-0.3）
- AMP占比会很高（> 90%）

当前情况: AMP占比 96.5%，说明使用的是旧配置
```
