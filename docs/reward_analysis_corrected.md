# 奖励分析（基于实际数据反推配置）

基于训练日志（iteration 1590）的实际数据，反推实际使用的配置参数。

## 一、训练数据

**日志数据**:
- Mean reward: 24.25
- Mean episode length: 640.79 步
- Episode_Reward/track_lin_vel_xy_exp: 0.9174
- Episode_Reward/track_ang_vel_z_exp: 0.7477
- 其他奖励项（惩罚项）总和: -0.4339

**当前配置文件**:
- `amp_reward_coef = 0.04`
- `amp_task_reward_lerp = 1.0` (100% Task, 0% AMP)

**注意**: 训练日志可能使用的是旧配置，需要反推实际使用的配置。

---

## 二、基于数据反推配置

### 2.1 计算每步奖励

```
每步平均奖励 = 24.25 / 640.79 ≈ 0.0378
每步Task奖励 = 1.2312 / 640.79 ≈ 0.00192
```

### 2.2 反推 AMP 奖励和配置

**假设使用配置**: `amp_task_reward_lerp = X`, `amp_reward_coef = Y`

**公式**:
```
最终奖励 = (1-X) × AMP奖励 + X × Task奖励
0.0378 = (1-X) × AMP奖励 + X × 0.00192
```

**情况1: 如果 `amp_task_reward_lerp = 0.7`（旧配置）**
```
0.0378 = 0.3 × AMP奖励 + 0.7 × 0.00192
0.0378 = 0.3 × AMP奖励 + 0.001344
AMP奖励 = (0.0378 - 0.001344) / 0.3 ≈ 0.1215
```

**情况2: 如果 `amp_task_reward_lerp = 0.85`（我之前修改的）**
```
0.0378 = 0.15 × AMP奖励 + 0.85 × 0.00192
0.0378 = 0.15 × AMP奖励 + 0.001632
AMP奖励 = (0.0378 - 0.001632) / 0.15 ≈ 0.241
```

**情况3: 如果 `amp_task_reward_lerp = 1.0`（当前配置，100% Task）**
```
0.0378 = 0.0 × AMP奖励 + 1.0 × 0.00192
0.0378 = 0.00192  ❌ 不匹配！
```

**结论**: 如果 `amp_task_reward_lerp = 1.0`，最终奖励应该等于 Task 奖励（0.00192），但实际是 0.0378，说明**训练日志使用的是旧配置**。

---

## 三、确定实际使用的配置

### 3.1 反推 `amp_task_reward_lerp`

**从公式反推**:
```
0.0378 = (1-X) × AMP奖励 + X × 0.00192
```

**假设 AMP 奖励范围**: 如果 `amp_reward_coef = 0.2`，AMP 奖励最大约 0.2

**如果 AMP 奖励 = 0.1215**:
```
0.0378 = (1-X) × 0.1215 + X × 0.00192
0.0378 = 0.1215 - 0.1215X + 0.00192X
0.0378 = 0.1215 - 0.11958X
0.11958X = 0.1215 - 0.0378 = 0.0837
X ≈ 0.7
```

**如果 AMP 奖励 = 0.241**:
```
0.0378 = (1-X) × 0.241 + X × 0.00192
0.0378 = 0.241 - 0.241X + 0.00192X
0.0378 = 0.241 - 0.23908X
0.23908X = 0.241 - 0.0378 = 0.2032
X ≈ 0.85
```

### 3.2 最可能的配置

**基于数据反推，最可能的情况**:

| 配置 | AMP奖励 | Task权重 | 匹配度 |
|------|---------|----------|--------|
| `lerp=0.7, coef=0.2` | 0.1215 | 0.7 | ✅ 可能 |
| `lerp=0.85, coef=0.2` | 0.241 | 0.85 | ✅ 可能 |
| `lerp=1.0, coef=0.04` | - | 1.0 | ❌ 不匹配 |

**判断**: 
- 如果 `amp_reward_coef = 0.2`，AMP 奖励最大约 0.2
- 实际 AMP 奖励约 0.12-0.24，说明可能使用了 `coef=0.2` 或 `coef=0.3`
- 最可能是 `lerp=0.7` 或 `lerp=0.85`，而不是 `lerp=1.0`

---

## 四、修正后的分析

### 4.1 如果使用 `lerp=0.7, coef=0.2`

| 奖励类型 | 每步值 | 权重 | 贡献值 | 占比 |
|---------|--------|------|--------|------|
| Task奖励 | 0.00192 | 0.7 | 0.001344 | **3.6%** |
| AMP奖励 | 0.1215 | 0.3 | 0.03645 | **96.4%** |

### 4.2 如果使用 `lerp=0.85, coef=0.2`

| 奖励类型 | 每步值 | 权重 | 贡献值 | 占比 |
|---------|--------|------|--------|------|
| Task奖励 | 0.00192 | 0.85 | 0.001632 | **4.3%** |
| AMP奖励 | 0.241 | 0.15 | 0.036168 | **95.7%** |

### 4.3 结论

**无论使用哪种配置，AMP 奖励都占主导（95-96%）！**

- ✅ Task 奖励太小（0.00192/步）
- ✅ AMP 奖励太大（0.12-0.24/步）
- ✅ 即使 Task 权重 70% 或 85%，实际贡献仍然很小

---

## 五、如何确认实际配置？

### 方法1: 查看训练日志的启动参数

训练脚本启动时会打印配置，检查日志开头部分。

### 方法2: 查看检查点保存的配置

检查点文件中可能保存了训练时的配置。

### 方法3: 基于数据反推（当前方法）

从奖励数据反推配置，但可能有多种可能性。

---

## 六、总结

### 6.1 关键发现

1. **我在分析中假设了 `lerp=0.85`**，这是基于我之前修改的配置
2. **但训练日志可能使用的是不同的配置**（可能是 0.7 或 0.85）
3. **无论使用哪种配置，AMP 奖励都占主导（95-96%）**

### 6.2 实际配置的确定

**需要确认**:
- 训练日志（iteration 1590）实际使用的配置
- 可以通过查看训练启动日志或检查点文件确认

### 6.3 当前配置文件

**当前配置文件显示**:
- `amp_reward_coef = 0.04`
- `amp_task_reward_lerp = 1.0` (100% Task)

**如果使用这个配置**:
- 最终奖励应该 = Task 奖励 = 0.00192
- 但实际是 0.0378，说明**训练日志使用的是旧配置**

---

## 七、建议

1. **确认训练日志使用的配置**
   - 查看训练启动日志
   - 或查看检查点文件中的配置

2. **如果使用旧配置（lerp=0.7 或 0.85）**
   - 需要重新训练，使用新配置（lerp=1.0, coef=0.04）

3. **如果使用新配置（lerp=1.0）**
   - 但奖励数据不匹配，说明配置可能没有生效
   - 需要检查配置加载逻辑
