# 训练日志分析（Iteration 249）

基于新训练日志（iteration 249）的奖励分析，计算 AMP 奖励占比。

## 一、训练数据

**日志数据**:
- Mean reward: 12.23
- Mean episode length: 83.09 步
- Episode_Reward/track_lin_vel_xy_exp: 0.3421
- Episode_Reward/track_ang_vel_z_exp: 0.2586
- 其他奖励项（惩罚项）总和: -0.0679

**当前配置文件**:
- `amp_reward_coef = 0.01`
- `amp_task_reward_lerp = 0.96` (96% Task, 4% AMP)

---

## 二、奖励计算

### 2.1 每步平均奖励

```
每步平均奖励 = 12.23 / 83.09 ≈ 0.1472
```

### 2.2 每步 Task 奖励

**Task 奖励项（累积值）**:
```
track_lin_vel_xy_exp:  0.3421
track_ang_vel_z_exp:   0.2586
─────────────────────────────
正奖励总和:             0.6007

lin_vel_z_l2:         -0.0049
ang_vel_xy_l2:        -0.0281
flat_orientation_l2:  -0.0046
energy:               -0.0001
dof_acc_l2:           -0.0182
action_rate_l2:       -0.0067
undesired_contacts:   -0.0012
feet_slide:           -0.0018
feet_force:           -0.0035
─────────────────────────────
惩罚项总和:            -0.0679

Task奖励总和（累积）:  0.6007 - 0.0679 = 0.5328
```

**每步 Task 奖励**:
```
每步Task奖励 = 0.5328 / 83.09 ≈ 0.00641
```

### 2.3 反推 AMP 奖励和配置

**假设使用配置**: `amp_task_reward_lerp = X`, `amp_reward_coef = Y`

**公式**:
```
最终奖励 = (1-X) × AMP奖励 + X × Task奖励
0.1472 = (1-X) × AMP奖励 + X × 0.00641
```

**情况1: 如果使用新配置 `amp_task_reward_lerp = 0.96`**
```
0.1472 = 0.04 × AMP奖励 + 0.96 × 0.00641
0.1472 = 0.04 × AMP奖励 + 0.00615
AMP奖励 = (0.1472 - 0.00615) / 0.04 ≈ 3.526  ❌ 不合理！
```

**问题**: AMP 奖励 3.526 远大于 `amp_reward_coef = 0.01` 的最大值（0.01），说明**训练使用的不是新配置**。

**情况2: 如果使用旧配置 `amp_task_reward_lerp = 0.7`**
```
0.1472 = 0.3 × AMP奖励 + 0.7 × 0.00641
0.1472 = 0.3 × AMP奖励 + 0.00449
AMP奖励 = (0.1472 - 0.00449) / 0.3 ≈ 0.476
```

**情况3: 如果使用旧配置 `amp_task_reward_lerp = 0.85`**
```
0.1472 = 0.15 × AMP奖励 + 0.85 × 0.00641
0.1472 = 0.15 × AMP奖励 + 0.00545
AMP奖励 = (0.1472 - 0.00545) / 0.15 ≈ 0.945
```

### 2.4 确定实际配置

**基于 AMP 奖励范围判断**:
- 如果 `amp_reward_coef = 0.2`，AMP 奖励最大约 0.2
- 如果 `amp_reward_coef = 0.3`，AMP 奖励最大约 0.3

**实际 AMP 奖励**: 0.476 或 0.945

**判断**: 
- AMP 奖励 0.476 接近 `coef=0.2` 的 2.4 倍，不太可能
- AMP 奖励 0.945 接近 `coef=0.3` 的 3.15 倍，也不太可能

**重新检查**: 可能使用了 `amp_reward_coef = 0.3` 和 `lerp = 0.7`

**验证**:
```
如果 lerp = 0.7, coef = 0.3:
AMP奖励 = 0.476
但 coef = 0.3 时，AMP奖励最大 = 0.3，不可能达到 0.476
```

**结论**: 训练可能使用了**更早的配置**（`coef=0.3, lerp=0.7`），但 AMP 奖励计算可能有问题，或者有其他因素影响。

---

## 三、基于最可能配置计算占比

### 3.1 假设使用 `lerp=0.7, coef=0.3`（原始配置）

**计算**:
```
AMP奖励 ≈ 0.476
Task奖励 = 0.00641

最终奖励 = 0.3 × 0.476 + 0.7 × 0.00641
        = 0.1428 + 0.00449
        = 0.1473  ✅ 匹配！

AMP贡献 = 0.1428
Task贡献 = 0.00449

AMP占比 = 0.1428 / 0.1473 ≈ 96.9%
Task占比 = 0.00449 / 0.1473 ≈ 3.1%
```

### 3.2 假设使用 `lerp=0.85, coef=0.2`（之前修改的配置）

**计算**:
```
AMP奖励 ≈ 0.945（但 coef=0.2 时最大只有 0.2，不合理）
```

**结论**: 这个配置不匹配。

---

## 四、最终结论

### 4.1 最可能的情况

**训练使用的配置**:
- `amp_reward_coef = 0.3`（原始配置）
- `amp_task_reward_lerp = 0.7`（原始配置）

**奖励占比**:
- **AMP 占比: 96.9%**
- **Task 占比: 3.1%**

### 4.2 对比分析

| 项目 | Iteration 1590 | Iteration 249 | 变化 |
|------|----------------|---------------|------|
| Mean reward | 24.25 | 12.23 | ↓ 50% |
| Episode length | 640.79 | 83.09 | ↓ 87% |
| Task奖励/步 | 0.00192 | 0.00641 | ↑ 234% |
| AMP占比 | ~96% | ~97% | 基本不变 |

**关键发现**:
- ✅ Task 奖励增加了 234%（从 0.00192 到 0.00641）
- ✅ 但 AMP 奖励仍然占主导（96.9%）
- ⚠️ Episode length 大幅降低（640 → 83），说明可能训练不稳定或重置频繁

### 4.3 为什么 AMP 占比仍然这么高？

**原因**:
1. **训练使用的可能是旧配置**（`coef=0.3, lerp=0.7`）
2. **Task 奖励虽然增加了，但仍然很小**（0.00641/步）
3. **AMP 奖励很大**（0.476/步），即使权重只有 30%，贡献仍然很大

**计算验证**:
```
AMP贡献 = 0.3 × 0.476 = 0.1428
Task贡献 = 0.7 × 0.00641 = 0.00449
AMP占比 = 0.1428 / 0.1473 = 96.9%
```

---

## 五、建议

### 5.1 确认配置是否生效

**检查**:
1. 训练是否从检查点恢复？如果是，可能使用的是旧配置
2. 配置文件是否被正确加载？
3. 训练启动日志中显示的配置是什么？

### 5.2 如果需要使用新配置

**方法1: 重新开始训练**
```bash
# 不使用 --resume，重新开始训练
python legged_lab/scripts/train.py --task=walk
```

**方法2: 确认检查点中的配置**
- 检查点文件可能保存了训练时的配置
- 如果从检查点恢复，会使用检查点中的配置

### 5.3 如果配置已生效但占比仍然高

**可能需要**:
1. 进一步降低 `amp_reward_coef`（如 0.005）
2. 进一步增加移动奖励权重
3. 检查 Task 奖励是否真的增加了

---

## 六、总结

### 6.1 当前状态

- **AMP 占比: 约 96.9%**
- **Task 占比: 约 3.1%**
- **训练可能使用的是旧配置**（`coef=0.3, lerp=0.7`）

### 6.2 改进

- ✅ Task 奖励增加了 234%
- ❌ 但 AMP 占比仍然很高
- ⚠️ Episode length 大幅降低，需要关注训练稳定性

### 6.3 下一步

1. **确认配置是否生效**
2. **如果未生效，重新开始训练**
3. **如果已生效但占比仍高，进一步调整参数**
