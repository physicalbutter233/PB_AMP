# 当前训练奖励分析

基于训练日志（iteration 1590）的奖励分析。

## 一、训练数据

**日志数据**:
- Mean reward: 24.25
- Mean episode length: 640.79 步
- Episode_Reward/track_lin_vel_xy_exp: 0.9174
- Episode_Reward/track_ang_vel_z_exp: 0.7477
- 其他奖励项（惩罚项）总和: -0.4339

**配置参数**:
- `amp_reward_coef = 0.2`
- `amp_task_reward_lerp = 0.85` (85% Task, 15% AMP)

---

## 二、奖励计算

### 2.1 每步平均奖励

```
每步平均奖励 = 24.25 / 640.79 ≈ 0.0378
```

### 2.2 每步 Task 奖励

**Task 奖励项（累积值）**:
```
track_lin_vel_xy_exp:  0.9174
track_ang_vel_z_exp:   0.7477
─────────────────────────────
正奖励总和:             1.6651

lin_vel_z_l2:         -0.0109
ang_vel_xy_l2:        -0.1304
flat_orientation_l2:   -0.0547
energy:               -0.0006
dof_acc_l2:           -0.1121
action_rate_l2:       -0.0810
undesired_contacts:   -0.0007
feet_slide:           -0.0141
feet_force:           -0.0294
─────────────────────────────
惩罚项总和:            -0.4339

Task奖励总和（累积）:  1.6651 - 0.4339 = 1.2312
```

**每步 Task 奖励**:
```
每步Task奖励 = 1.2312 / 640.79 ≈ 0.00192
```

### 2.3 每步 AMP 奖励

**根据公式**:
```
最终奖励 = 0.15 × AMP奖励 + 0.85 × Task奖励
0.0378 = 0.15 × AMP奖励 + 0.85 × 0.00192
0.0378 = 0.15 × AMP奖励 + 0.001632
AMP奖励 = (0.0378 - 0.001632) / 0.15
AMP奖励 ≈ 0.241
```

### 2.4 奖励占比

**在最终奖励中的占比**:

| 奖励类型 | 每步值 | 权重 | 贡献值 | 占比 |
|---------|--------|------|--------|------|
| Task奖励 | 0.00192 | 0.85 | 0.001632 | **4.3%** |
| AMP奖励 | 0.241 | 0.15 | 0.036168 | **95.7%** |
| **总计** | - | - | **0.0378** | **100%** |

---

## 三、问题分析

### 3.1 关键发现

**AMP 奖励仍然占主导地位（95.7%）！**

尽管配置中：
- `amp_task_reward_lerp = 0.85`（85% Task 权重）
- `amp_reward_coef = 0.2`（降低 AMP 奖励）

但实际效果：
- Task 奖励贡献：仅 4.3%
- AMP 奖励贡献：95.7%

### 3.2 原因分析

**为什么 Task 权重 85%，但贡献只有 4.3%？**

**原因**：Task 奖励本身太小（0.00192/步），即使权重 85%，贡献也很小。

**计算验证**:
```
Task贡献 = 0.85 × 0.00192 = 0.001632
AMP贡献 = 0.15 × 0.241 = 0.036168

Task占比 = 0.001632 / 0.0378 ≈ 4.3%
AMP占比 = 0.036168 / 0.0378 ≈ 95.7%
```

**结论**：
- ✅ 权重设置正确（85% Task）
- ❌ 但 Task 奖励太小，导致实际贡献很小
- ❌ AMP 奖励太大（0.241），即使权重只有 15%，贡献仍然很大

---

## 四、对比分析

### 4.1 修改前后对比

| 项目 | 修改前 | 修改后（当前） | 变化 |
|------|--------|----------------|------|
| **配置** |
| amp_reward_coef | 0.3 | 0.2 | ↓ 33% |
| amp_task_reward_lerp | 0.7 | 0.85 | ↑ 21% |
| track_lin_vel weight | 1.2 | 2.5 | ↑ 108% |
| track_ang_vel weight | 1.1 | 2.0 | ↑ 82% |
| **实际奖励** |
| Task奖励/步 | 0.0014 | 0.00192 | ↑ 37% |
| AMP奖励/步 | 0.187 | 0.241 | ↑ 29% |
| 最终奖励/步 | 0.057 | 0.0378 | ↓ 34% |
| **占比** |
| Task占比 | 2.4% | 4.3% | ↑ 79% |
| AMP占比 | 97.6% | 95.7% | ↓ 2% |

### 4.2 改进效果

**有改进，但还不够**：
- ✅ Task 奖励增加了 37%
- ✅ Task 占比从 2.4% 提升到 4.3%（几乎翻倍）
- ❌ 但 AMP 奖励仍然占主导（95.7%）
- ❌ AMP 奖励甚至增加了（0.187 → 0.241）

**为什么 AMP 奖励增加了？**
- 可能是因为策略学习得更好，动作更接近专家
- 判别器输出 `d` 更接近 1，导致 AMP 奖励增加

---

## 五、进一步优化建议

### 方案1：进一步降低 AMP 奖励系数

**当前**: `amp_reward_coef = 0.2`

**建议**: `amp_reward_coef = 0.1` 或 `0.05`

**预期效果**:
```
AMP奖励 ≈ 0.241 × (0.1 / 0.2) ≈ 0.12
最终奖励 = 0.15 × 0.12 + 0.85 × 0.00192 ≈ 0.0196
Task占比 = 0.85 × 0.00192 / 0.0196 ≈ 8.3%
```

### 方案2：进一步增加移动奖励权重

**当前**: 
- `track_lin_vel_xy_exp.weight = 2.5`
- `track_ang_vel_z_exp.weight = 2.0`

**建议**: 
- `track_lin_vel_xy_exp.weight = 4.0`
- `track_ang_vel_z_exp.weight = 3.0`

**预期效果**:
```
Task奖励 ≈ 0.00192 × (4.0/2.5) ≈ 0.00307
最终奖励 = 0.15 × 0.241 + 0.85 × 0.00307 ≈ 0.0402
Task占比 = 0.85 × 0.00307 / 0.0402 ≈ 6.5%
```

### 方案3：组合方案（最推荐）

**同时调整**:
```python
# 1. 进一步降低 AMP 奖励
amp_reward_coef = 0.1  # 0.2 → 0.1

# 2. 进一步增加移动奖励权重
track_lin_vel_xy_exp.weight = 4.0  # 2.5 → 4.0
track_ang_vel_z_exp.weight = 3.0   # 2.0 → 3.0

# 3. 可选：进一步增加 Task 权重
amp_task_reward_lerp = 0.9  # 0.85 → 0.9
```

**预期效果**:
```
Task奖励 ≈ 0.00192 × (4.0/2.5) ≈ 0.00307
AMP奖励 ≈ 0.241 × (0.1/0.2) ≈ 0.12
最终奖励 = 0.1 × 0.12 + 0.9 × 0.00307 ≈ 0.0148
Task占比 = 0.9 × 0.00307 / 0.0148 ≈ 18.7%
```

---

## 六、当前状态评估

### 6.1 训练效果

**从日志看**:
- ✅ `track_lin_vel_xy_exp: 0.9174` - 比之前（0.4794）提升了 91%
- ✅ `track_ang_vel_z_exp: 0.7477` - 比之前（0.6569）提升了 14%
- ✅ Mean episode length: 640.79 - 比之前（802.98）降低了，说明可能开始移动了
- ✅ Mean reward: 24.25 - 比之前（45.99）降低了，符合预期

**判断**:
- ✅ 移动相关奖励增加了
- ✅ 机器人可能开始移动了
- ⚠️ 但 AMP 奖励仍然占主导

### 6.2 是否需要进一步调整？

**如果机器人已经移动**:
- ✅ 可以保持当前配置，继续训练
- ✅ 观察训练进展，看是否继续改善

**如果机器人仍然不移动**:
- ❌ 需要进一步降低 AMP 奖励
- ❌ 需要进一步增加移动奖励权重

---

## 七、总结

### 7.1 关键发现

1. **AMP 奖励仍然占主导（95.7%）**
   - 尽管配置中 Task 权重 85%
   - 但 Task 奖励太小，实际贡献只有 4.3%

2. **有改进，但不够**
   - Task 占比从 2.4% 提升到 4.3%
   - 移动奖励增加了 91%
   - 但 AMP 奖励仍然太大

3. **需要进一步调整**
   - 降低 `amp_reward_coef` 到 0.1 或 0.05
   - 增加移动奖励权重到 4.0 和 3.0
   - 可选：增加 `amp_task_reward_lerp` 到 0.9

### 7.2 建议

**如果机器人已经移动**:
- ✅ 保持当前配置，继续训练
- ✅ 监控训练进展

**如果机器人仍然不移动**:
- ❌ 应用方案3（组合方案）
- ❌ 进一步降低 AMP 奖励
- ❌ 进一步增加移动奖励权重
